<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo建站后注意事项</title>
    <url>/2022/09/05/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如何使用Hexo建站，官网也有很多教程，这里不再赘述。详见-&gt; <a href="https://hexo.io/zh-cn/">Hexo官方网站</a></p>
<p>里面关于如何建站的内容很清晰，如何更换主题，也有详尽的描述。</p>
<p>但关于一些博客发布过程中的常见问题，却缺少解决方法。</p>
<h2 id="Hexo发布博客注意事项-amp-问题汇总"><a href="#Hexo发布博客注意事项-amp-问题汇总" class="headerlink" title="Hexo发布博客注意事项&amp;问题汇总"></a>Hexo发布博客注意事项&amp;问题汇总</h2><span id="more"></span>

<h3 id="1-快速保存Hexo本地博客源码"><a href="#1-快速保存Hexo本地博客源码" class="headerlink" title="1. 快速保存Hexo本地博客源码"></a>1. 快速保存Hexo本地博客源码</h3><p>本地博客的内容都是通过markdown或者其它文件格式保存在本地。而部署的时候，Hexo是帮忙把博客内容转化为了html然后再发布。</p>
<p>源码内容在： <code>C:\Users\cheery\OneDrive\04-Blog\cheery-chen</code></p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/%E6%BA%90%E7%A0%81.png"></p>
<p>部署内容在：<code>C:\Users\cheery\OneDrive\04-Blog\cheery-chen\.deploy_git </code></p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/%E9%83%A8%E7%BD%B2.png"></p>
<p>部署的时候，只会把<code>.deploy_git</code>中的内容发布，而源码内容还是保存在本地。</p>
<p>因此对源码内容的保存，这里有两个建议：</p>
<ol>
<li><p>把源码内容保存到自己github.io的分支中。</p>
<p>优点：全平台同步，哪里都可以download下来然后写博客。</p>
<p>缺点：操作稍复杂，如果不设置为私有库，谁都可以看到。</p>
</li>
<li><p>直接copy整个源码内容到本地的OneDrive文件夹中。（我喜欢这个）</p>
<p>优点：操作简单，只有自己可见源码，windows平台间同步。</p>
<p>缺点：只有windows平台同步，linux等平台支持不好。</p>
</li>
</ol>
<h3 id="2-快速预览和部署Hexo博客设置"><a href="#2-快速预览和部署Hexo博客设置" class="headerlink" title="2. 快速预览和部署Hexo博客设置"></a>2. 快速预览和部署Hexo博客设置</h3><p>每次预览和上传博客，都需要输入hexo clean, hexo g, hexo s, hexo d</p>
<p>这样不免效率太低，写一个非常简单的shell 脚本，只用一个命令就可以完成网站预览和部署。</p>
<p>首先进入到自己的hexo目录，创建一个shell脚本：</p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/shell%E8%84%9A%E6%9C%AC.png"></p>
<p>然后在blog.sh中，填入如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">pre_blog()</span><br><span class="line">&#123;</span><br><span class="line">	hexo clean</span><br><span class="line">	hexo g</span><br><span class="line">	hexo s</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">deploy_blog()</span><br><span class="line">&#123;</span><br><span class="line">	hexo clean</span><br><span class="line">	hexo g</span><br><span class="line">	hexo deploy</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [[ $1 == &quot;s&quot; ]]</span><br><span class="line">then</span><br><span class="line">	echo &quot;start preview blog!!!&quot;</span><br><span class="line">	pre_blog</span><br><span class="line">elif [[ $1 == &quot;d&quot; ]]</span><br><span class="line">then</span><br><span class="line">	echo &quot;deploy blog!!!&quot;</span><br><span class="line">	deploy_blog</span><br><span class="line">fi</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在自己的hexo目录下启动git bash：</p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/git_bash.png"></p>
<p>使用<code>./blog.sh s</code>  会自动<code>hexo clean&amp;generate&amp;server</code>, 然后在本地输入<a href="http://localhost:4000/">localhost:4000</a>进行预览自己的网页。如果看到没有问题后，<code>ctrl+c</code>停止当前预览。</p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/preview.png"></p>
<p>使用<code>./blog.sh d</code>进行自动部署（前提是配置好_config.yml中的deploy设置），会自动<code>hexo clean&amp;generate&amp;deploy</code> ，完成github的部署</p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/deploy.png"></p>
<p>个人<code>_config.yml</code>中的deploy配置，通过github进行挂载：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="comment">#repo: https://github.com/cheery-chen/cheery-chen.github.io.git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/自己的用户名/自己的用户名.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure>



<h3 id="3-如何将自己买的域名绑定到github-io上"><a href="#3-如何将自己买的域名绑定到github-io上" class="headerlink" title="3. 如何将自己买的域名绑定到github.io上"></a>3. 如何将自己买的域名绑定到github.io上</h3><p>当把自己的博客挂载到了cheery-chen.github.io后，又在阿里云上，够买了域名cheerychen.com绑定到cheery-chen.github.io</p>
<p>登录GitHub，进入之前创建的仓库，点击settings，找到Page页面 设置Custom domain，输入你的域名cheerychen.com</p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/custom_domain.png"></p>
<p>然后还需要在Hexo的Source文件夹下新建一个CNAME文件，里面填入cheerychen.com， 这样才不会每次hexo deploy之后，github上的custom domain地址被冲掉。</p>
<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/CNAME.png"></p>
<p>CNAME中的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cheerychen.com</span><br><span class="line">www.cheerychen.com</span><br></pre></td></tr></table></figure>

<p><img data-src="/images/Hexo%E5%BB%BA%E7%AB%99%E5%90%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/cheerychen.png"></p>
]]></content>
      <categories>
        <category>个人建站技巧</category>
      </categories>
      <tags>
        <tag>网站</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/09/03/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>网站</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN初解</title>
    <url>/2023/01/04/CNN%E5%88%9D%E8%A7%A3/</url>
    <content><![CDATA[<p>首先，神经网络（Neural Networks）并不复杂，仅仅是一个专业词汇（唬人词汇），事实上，它远比想象中的要简单很多。</p>
<p>前置需求：一点点线性代数，一点点Python基础（如果都不了解，也能看懂大概）。</p>
<h1 id="1-初识神经网络（Neural-Networks）"><a href="#1-初识神经网络（Neural-Networks）" class="headerlink" title="1. 初识神经网络（Neural Networks）"></a>1. 初识神经网络（Neural Networks）</h1><h2 id="1-1-建立模块：神经元（Neurons）"><a href="#1-1-建立模块：神经元（Neurons）" class="headerlink" title="1.1 建立模块：神经元（Neurons）"></a>1.1 建立模块：神经元（Neurons）</h2><p>​	首先神经网络的基本单元就是”神经元（Neurons）”。<strong>一个神经元的构成，首先有多个输入，然后对多个输入进行一些数学的运算，最后得到一个输出。</strong> 下图是一个典型的2个输入的神经元。</p>
<p><img data-src="https://cheery-markdown-img.oss-cn-chengdu.aliyuncs.com/blog/20230104151944.png" alt="神经元"></p>
<p>​	神经元中发生了三件事情：</p>
<ul>
<li><p>首先，每个输入乘以一个权重(Weight)：（图中红色方块）<br>$$<br>x_1 \rightarrow x_1 * w_1 \<br>x_2 \rightarrow x_2 * w_2<br>$$</p>
</li>
<li><p>然后，所有的乘了权重之后的输入加到一起，并再加上一个偏置（bias）b：（图中绿色方块)<br>$$<br>(x_1 * w_1) + (x_2 * w_2) + b<br>$$<br><strong>偏置b在此图中没有画出，后续学习中，可以理解偏置b就是一个权重固定为1的输入，有的教材中就将偏置b，直接等效为一个$$x_0$$乘以一个固定权重为1的输入</strong><br>$$<br>(x_0 * 1) + (x_1 * w_1) + (x_2 * w_2)<br>$$</p>
</li>
<li><p>最后，加到一起的和，经过一个激活函数：（图中橙色方块）<br>$$<br>y &#x3D; f(x_1 * w_1 + x_2 * w_2 + b)<br>$$<br>​	这个激活函数主要作用是将一个无界限的输入（多个输入乘以权重相加后，是一个没有确定的界限的值），变为具有良好、可预测形式的输出。如下图的例子，就是一个常用的sogmoid激活函数（当然还有很多其它类型的激活函数，这里不进一步展开）。</p>
<span id="more"></span></li>
</ul>
<p><img data-src="https://cheery-markdown-img.oss-cn-chengdu.aliyuncs.com/blog/20230104205249.png" alt="softmax函数"></p>
<p>​	这个sigmoid激活函数的输出范围是（0,1）。简单来说，这个激活函数的作用就是将一个可能为$$(-\infty, +\infty)$$范围的输入值（横坐标），转化为（0,1）的输出值（纵坐标）。输入越小的负数，经过sigmoid函数后，输出越接近于0；输入越大的正数，经过sigmoid函数后，输出越接近于1。</p>
<h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p>​	假设我们有一个神经元，具有2个输入，1个输出，使用sigmoid作为激活函数，具体的参数如下：<br>$$<br>w &#x3D; [0,1] \<br>b &#x3D; 4<br>$$<br>其中$$w&#x3D;[0,1]$$只是$$w_1 &#x3D; 0, w_2 &#x3D; 1$$的向量写法。</p>
<p>现在我们给神经元一个输入向量$$x &#x3D; [2,3]$$。我们使用更简洁的<strong>点乘（dot product）</strong>来表示：<br>$$<br>\begin{align}<br>(w \cdot x) + b &amp;&#x3D; ((x_1 * w_1) + (x_2 * w_2)) + b \<br>&amp;&#x3D; 0 * 2 + 1 * 3 + 4 \<br>&amp;&#x3D; 7<br>\end{align}<br>$$</p>
<p>$$<br>y &#x3D; f(w \cdot x + b) &#x3D; f(7) &#x3D; 0.999<br>$$</p>
<p>输入向量$$x &#x3D; [2,3]$$经过神经元后，得到0.999的输出。就是这么简单，这种向前传递输入（图中是从左向右传递），得到输出的过程称为<strong>前馈（feedforward）</strong>。</p>
<h3 id="编写一个简单的神经元"><a href="#编写一个简单的神经元" class="headerlink" title="编写一个简单的神经元"></a>编写一个简单的神经元</h3><p>​	我们使用Python中的Numpy包，它是一个很流行和强大的矩阵计算库，可以极大的方便进行数学矩阵运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="comment"># Our activation function: f(x) = 1 / (1 + e^(-x))</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weights, bias</span>):</span><br><span class="line">    self.weights = weights</span><br><span class="line">    self.bias = bias</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">    <span class="comment"># Weight inputs, add bias, then use the activation function</span></span><br><span class="line">    total = np.dot(self.weights, inputs) + self.bias</span><br><span class="line">    <span class="keyword">return</span> sigmoid(total)</span><br><span class="line"></span><br><span class="line">weights = np.array([<span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># w1 = 0, w2 = 1</span></span><br><span class="line">bias = <span class="number">4</span>                   <span class="comment"># b = 4</span></span><br><span class="line">n = Neuron(weights, bias)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">2</span>, <span class="number">3</span>])       <span class="comment"># x1 = 2, x2 = 3</span></span><br><span class="line"><span class="built_in">print</span>(n.feedforward(x))    <span class="comment"># 0.9990889488055994</span></span><br></pre></td></tr></table></figure>

<p>以上的python代码，就是上面举例子的代码实现，得到了相同的结果0.999。</p>
<h2 id="1-2-多个神经元组合成一个神经网络"><a href="#1-2-多个神经元组合成一个神经网络" class="headerlink" title="1.2 多个神经元组合成一个神经网络"></a>1.2 多个神经元组合成一个神经网络</h2><p>​	神经网络只不过是一堆连接在一起的神经元。下面就是一个简单的神经网络的样子（因为只有从左到右的传输，也叫<strong>前馈神经网络</strong>）：</p>
<p><img data-src="https://cheery-markdown-img.oss-cn-chengdu.aliyuncs.com/blog/20230105102832.png" alt="神经网络"></p>
<p>这个神经元有三层，第一层为输入层，具有两个输入，无神经元；第二层为隐藏层，具有两个神经元$$(h_1, h_2)$$；第三层为输出层，具有一个神经元$$o_1$$。注意这里$$o_1$$的两个输入是来自隐藏层的$$(h_1, h_2)$$–我们就叫这是一个网络。</p>
<p><strong>注意：隐藏层是输入层（第一层）和输出层（最后一层）中间的任何一层。也就是说隐藏层可以有多层。</strong></p>
<h3 id="例子：一个简单的前馈神经网络"><a href="#例子：一个简单的前馈神经网络" class="headerlink" title="例子：一个简单的前馈神经网络"></a>例子：一个简单的前馈神经网络</h3><p>​	让我们使用上图的网络，并假设所有神经元具有相同的权重$$w &#x3D; [0,1]$$，相同的偏置$$b &#x3D; 0$$，相同的sigmoid激活函数。最后让$$(h_1, h_2, o_1)$$代表对应神经元的输出。</p>
<p>如果我们给定输入$$x &#x3D; [2,3]$$，神经网络是怎么运转的呢？<br>$$<br>\begin{align}<br>h_1 &#x3D; h_2 &amp;&#x3D; f(w.x + b) \<br>&amp;&#x3D; f((0 * 2) + (1 * 3) + 0) \<br>&amp;&#x3D; f(3) \<br>&amp;&#x3D; 0.9526<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>o_1 &amp;&#x3D; f(w.[h_1, h_2] + b) \<br>&amp;&#x3D; f((0 * h_1) + (1 * h_2) + 0) \<br>&amp;&#x3D; f(0.9526) \<br>&amp;&#x3D; 0.7216<br>\end{align}<br>$$</p>
<p>因此，我们得到了输入$$x &#x3D; [2,3]$$经过神经网络后，得到输出为0.7216。这是简单易懂的过程。</p>
<p>神经网络可以有<strong>任意数量的层</strong>，这些层中可以有<strong>任意数量的神经元</strong>。但基本思想保持不变：通过网络中的神经元向前馈送输入，在最后获得输出。为简单起见，在本文的其余部分，我们将继续使用上图的网络。</p>
<h3 id="编写：一个简单的前馈神经网络"><a href="#编写：一个简单的前馈神经网络" class="headerlink" title="编写：一个简单的前馈神经网络"></a>编写：一个简单的前馈神经网络</h3><p>我们将上面前馈神经网络的例子，通过python代码呈现出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># ... code from previous section here</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OurNeuralNetwork</span>:</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  A neural network with:</span></span><br><span class="line"><span class="string">    - 2 inputs</span></span><br><span class="line"><span class="string">    - a hidden layer with 2 neurons (h1, h2)</span></span><br><span class="line"><span class="string">    - an output layer with 1 neuron (o1)</span></span><br><span class="line"><span class="string">  Each neuron has the same weights and bias:</span></span><br><span class="line"><span class="string">    - w = [0, 1]</span></span><br><span class="line"><span class="string">    - b = 0</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    weights = np.array([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    bias = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The Neuron class here is from the previous section</span></span><br><span class="line">    self.h1 = Neuron(weights, bias)</span><br><span class="line">    self.h2 = Neuron(weights, bias)</span><br><span class="line">    self.o1 = Neuron(weights, bias)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, x</span>):</span><br><span class="line">    out_h1 = self.h1.feedforward(x)</span><br><span class="line">    out_h2 = self.h2.feedforward(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The inputs for o1 are the outputs from h1 and h2</span></span><br><span class="line">    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_o1</span><br><span class="line"></span><br><span class="line">network = OurNeuralNetwork()</span><br><span class="line">x = np.array([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(network.feedforward(x)) <span class="comment"># 0.7216325609518421</span></span><br></pre></td></tr></table></figure>

<p>最后得到相同的输出结果0.7216</p>
<h2 id="1-3-训练一个神经网络，第一部分"><a href="#1-3-训练一个神经网络，第一部分" class="headerlink" title="1.3 训练一个神经网络，第一部分"></a>1.3 训练一个神经网络，第一部分</h2><p>我们假设有如下的测量参数：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Weight (lb)(磅)</th>
<th>Height (in)(英寸)</th>
<th>Gender</th>
</tr>
</thead>
<tbody><tr>
<td>Alice</td>
<td>133</td>
<td>65</td>
<td>F</td>
</tr>
<tr>
<td>Bob</td>
<td>160</td>
<td>72</td>
<td>M</td>
</tr>
<tr>
<td>Charlie</td>
<td>152</td>
<td>70</td>
<td>M</td>
</tr>
<tr>
<td>Diana</td>
<td>120</td>
<td>60</td>
<td>F</td>
</tr>
</tbody></table>
<p>让我们来训练网络，通过给定体重和身高，来预测某人的性别：</p>
<p><img data-src="https://cheery-markdown-img.oss-cn-chengdu.aliyuncs.com/blog/20230105201640.png" alt="性别预测网络"></p>
<p>这里假设男生输出0，女生输出1，以及其它测量参数如下（这里选体重和身高随意选择了一个合适的值135磅和66英寸，这两个值也可以调整，主要是方便运算）：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Weight (minus 135)</th>
<th>Height (minus 66)</th>
<th>Gender</th>
</tr>
</thead>
<tbody><tr>
<td>Alice</td>
<td>-2</td>
<td>-1</td>
<td>1</td>
</tr>
<tr>
<td>Bob</td>
<td>25</td>
<td>6</td>
<td>0</td>
</tr>
<tr>
<td>Charlie</td>
<td>17</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>Diana</td>
<td>-15</td>
<td>-6</td>
<td>1</td>
</tr>
</tbody></table>
<p>表格中的体重和身高是减去合适的值（自己随意定）之后的值，比如Alice的体重-2，是由133（Alice自己的体重）减去135（体重中位数）得到的-2。</p>
<h3 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h3><p>在开始训练神经网络之前，我们还需要一种方法来量化它做得有多“好”，以便它可以尝试做得“更好”。这就是损失函数的由来，也可以简称为损失（Loss）。</p>
<p>这里，我们使用<strong>均方差（Mean Squared Error, MSE）</strong>作为<strong>损失函数</strong>来衡量网络的好坏。</p>
<p>当然还有很多其它的损失函数，比如交叉熵（Cross Entropy）详见：<a href="https://zhuanlan.zhihu.com/p/77686118">常见损失函数介绍</a></p>
<p>均方差（MSE）损失函数：<br>$$<br>MSE &#x3D; \frac{1}{n} \sum_{i &#x3D; 1}^{n} (y_{true} - y_{pred})^2<br>$$<br>其中，$$n$$是样本的个数，这里是4(4表示Alice, Bob, Charlie, Diana)；$$y$$表示性别，其中$$y_{true}$$表示真实的性别（比如Alice的$$y_{true} &#x3D; 1$$），$$y_{pred}$$表示神经网络预测给出的性别。</p>
<p>上述公式中$$(y_{true} - y_{pred})^2$$是我们熟知的<strong>平方误差（squared error）</strong>。我们的均方差损失函数就是取所有平方误差的平均值（因此得名<strong>均方差</strong>）。所以我们的预测越好，我们的损失（均方差）就越低！</p>
<p><strong>更好的预测 &#x3D; 更低的误差（更小的均方差）</strong></p>
<p><strong>训练一个好的神经网络 &#x3D; 让神经网络的损失尽量小</strong></p>
<h3 id="例子：均方差损失函数"><a href="#例子：均方差损失函数" class="headerlink" title="例子：均方差损失函数"></a>例子：均方差损失函数</h3><p>这里先假设，我们神经网络的输出一直为0，换句话说，就是让神经网络一直预测这个人为男生，最后的损失应该是什么？</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>$$y_{true}$$</th>
<th>$$y_{pred}$$</th>
<th>$$(y_{true} - y_{pred})^2$$</th>
</tr>
</thead>
<tbody><tr>
<td>Alice</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Bob</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Charlie</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Diana</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>$$<br>MSE &#x3D; \frac{1}{4}(1 + 0 + 0 + 1) &#x3D; 0.5<br>$$</p>
<h3 id="编写：均方差损失函数"><a href="#编写：均方差损失函数" class="headerlink" title="编写：均方差损失函数"></a>编写：均方差损失函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">  <span class="comment"># y_true and y_pred are numpy arrays of the same length.</span></span><br><span class="line">  <span class="keyword">return</span> ((y_true - y_pred) ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">y_true = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mse_loss(y_true, y_pred)) <span class="comment"># 0.5</span></span><br></pre></td></tr></table></figure>

<p>如果有疑惑，也可以看看numpy的官方教程：<a href="https://numpy.org/doc/stable/user/quickstart.html#basic-operations">quickstart</a> </p>
<h2 id="1-4-训练一个神经网络，第二部分"><a href="#1-4-训练一个神经网络，第二部分" class="headerlink" title="1.4 训练一个神经网络，第二部分"></a>1.4 训练一个神经网络，第二部分</h2><p>当了解到损失函数后，我们现在有了一个清晰的目标：<strong>让神经网络的损失最小。</strong></p>
<p>然后前面我们已经知道改变神经网络的权重和偏置，就可以影响神经网络的输出（预测），但是应该怎么来减少损失呢？</p>
<blockquote>
<p>这部分会有一点点多变量微积分。如果不感兴趣，可以直接跳过数学计算部分。</p>
</blockquote>
<p>为简单起见，让我们假设数据集中只有 Alice：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Weight (minus 135)</th>
<th>Height (minus 66)</th>
<th>Gender</th>
</tr>
</thead>
<tbody><tr>
<td>Alice</td>
<td>-2</td>
<td>-1</td>
<td>1</td>
</tr>
</tbody></table>
<p>均方差损失就只是Alice的平方误差：<br>$$<br>\begin{align}<br>MSE &amp;&#x3D; \frac{1}{1} \sum_{i &#x3D; 1}^{n} (y_{true} - y_{pred})^2 \<br>&amp;&#x3D; (y_{true} - y_{pred})^2 \<br>&amp;&#x3D; (1 - y_{pred})^2<br>\end{align}<br>$$<br>另一种计算损失的方法是，考虑一个权重和偏差的函数。让我们标记网络中的每个权重和偏差：</p>
<p><img data-src="https://cheery-markdown-img.oss-cn-chengdu.aliyuncs.com/blog/20230105221004.png" alt="带权重和偏置的神经网络"></p>
<p>然后，我们可以将损失写成一个多变量函数：<br>$$<br>L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3, b_4)<br>$$<br>想象一下，$$w_1$$的值发生改变， 会对损失$$L$$造成什么影响呢？损失$$L$$应该怎么变化呢？在换句话说，这个损失$$L$$，由$$w_1$$贡献了多少呢？这里就会用到<a href="https://baike.baidu.com/item/%E5%81%8F%E5%AF%BC%E6%95%B0/5536984">偏导数</a>: $$\frac{\partial{L}}{\partial{w_1}}$$。</p>
<p>具体如何计算损失呢？</p>
<blockquote>
<p>这里计算会相对复杂一些，但这也是<strong>神经网络的核心</strong>，花点时间，使用纸笔画一画，帮助理解。</p>
</blockquote>
<p>让我们使用<a href="https://baike.baidu.com/item/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/3314017">链式法则</a>把$$\frac{\partial{L}}{\partial{w_1}}$$变一下形：<br>$$<br>\frac{\partial{L}}{\partial{w_1}} &#x3D; \frac{\partial{L}}{\partial{y_{pred}}} * \frac{\partial{y_{pred}}}{\partial{w_1}}<br>$$<br>其中$$\frac{\partial{L}}{\partial{y_{pred}}}$$我们已经可以计算了，因为在前面已经计算过$$L &#x3D; (1 - y_{pred})^2$$，那么$$\frac{\partial{L}}{\partial{y_{pred}}}$$：<br>$$<br>\frac{\partial{L}}{\partial{y_{pred}}} &#x3D;  \frac{\partial{(1 - y_{pred})^2}}{\partial{y_{pred}}} &#x3D; \boxed{-2(1 - y_{pred})}<br>$$<br>然后，我们要考虑如何计算$$\frac{\partial{y_{pred}}}{\partial{w_1}}$$。和前面的方式一样，我们让$$h_1, h_2, o_1$$作为神经元的输出：<br>$$<br>y_{pred} &#x3D; o_1 &#x3D; f(w_5  h_1 + w_6  h_2 + b_3) \\<br>这里的激活函数f是sigmoid函数<br>$$<br>再然后，由于$$w_1$$只会影响到$$h_1$$，因此我们再继续使用链式法则将$$\frac{\partial{y_{pred}}}{\partial{w_1}}$$变一下形：<br>$$<br>\frac{\partial{y_{pred}}}{\partial{w_1}} &#x3D; \frac{\partial{y_{pred}}}{\partial{h_1}} * \frac{\partial{h_1}}{\partial{w_1}}<br>$$</p>
<p>$$<br>\frac{\partial{y_{pred}}}{\partial{h_1}} &#x3D; \boxed{w_5 * f’(w_1 x_1 + w_2 x_2 + b_1)}<br>$$</p>
<p>继续，对$$\frac{\partial{h_1}}{\partial{w_1}}$$链式法则:<br>$$<br>h_1 &#x3D; f(w_1 x_1 + w_2 x_2 + b1)<br>$$</p>
<p>$$<br>\frac{\partial{h_1}}{\partial{w_1}} &#x3D; \boxed{x1 * f’(w_1 x_1 + w_2 x_2 + b1)}<br>$$</p>
<p>上式中，$$x_1$$是体重，$$x_2$$是身高。然后，我们推导$$f’(x)$$，因为已经看到两次了。它本质是有sigmoid激活函数来的：<br>$$<br>f(x) &#x3D; \frac{1}{1 + e^{-x}} \<br>f’(x) &#x3D; \frac{e^{-x}}{(1 + e^{-x})^2} &#x3D; \boxed{f(x) * (1 - f(x)}<br>$$<br>后续会使用到$$f’(x)$$的等价公式。</p>
<p>最后，我们成功将$$\frac{\partial{L}}{\partial{w_1}}$$使用链式法则，分解为了几个部分，让我们可以计算：<br>$$<br>\boxed{\frac{\partial{L}}{\partial{w_1}} &#x3D; \frac{\partial{L}}{\partial{y_{pred}}} * \frac{\partial{y_{pred}}}{\partial{h_1}} * \frac{\partial{h_1}}{\partial{w_1}}}<br>$$<br>这个神经网络系统中，从最右边的输出层得到的损失函数，向最左边的输入层计算偏导的过程（从右向左进行计算，注意和前馈的概念分开），我们称之为<strong>反向传播（backpropagation，简称BP算法）</strong></p>
<blockquote>
<p>前馈神经网络的训练方法有很多，使用BP算法进行训练的前馈神经网络，又叫做BP网络（效果好，受欢迎）。</p>
<p>其它常见的<a href="https://baike.baidu.com/item/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/7580523">前馈神经网络</a>，还有比如感知机网络、径向基函数（Radial Basis Function, RBF）网络，仅做了解。</p>
</blockquote>
<p>好了，虽然有很多公式，可能有点绕，但下面直接通过一个例子来看具体是怎么计算这些偏导的吧！</p>
<h3 id="例子：计算偏导"><a href="#例子：计算偏导" class="headerlink" title="例子：计算偏导"></a>例子：计算偏导</h3><p>这里继续使用前面Alice的例子作为我们的数据集：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Weight (minus 135)</th>
<th>Height (minus 66)</th>
<th>Gender</th>
</tr>
</thead>
<tbody><tr>
<td>Alice</td>
<td>-2</td>
<td>-1</td>
<td>1</td>
</tr>
</tbody></table>
<p>然后我们初始化所有的权重$$w$$为1，所有的偏置$$b$$为0。然后我们做一次前向传播，经过神经网络后，得到：<br>$$<br>\begin{align}<br>h_1 &amp;&#x3D; f(w_1 x_1 + w_2 x_2 + b_1) \<br>&amp;&#x3D; f(-2 + -1 + 0) \<br>&amp;&#x3D; 0.0474<br>\end{align}<br>$$</p>
<p>$$<br>h_2 &#x3D; f(w_3 x_1 + w_4 x_2 + b_2) &#x3D; 0.0474<br>$$</p>
<p>$$<br>\begin{align}<br>o_1 &amp;&#x3D; f(w_5 h_1 + w6 h_2 + b_3) \<br>&amp;&#x3D; f(0.0474 + 0.0474 + 0) \<br>&amp;&#x3D; 0.524<br>\end{align}<br>$$</p>
<p>因此，计算得到神经网络的预测输出为$$y_{pred} &#x3D; 0.524$$，这表示没有对性别没有一个明显的偏向，既有可能为男生（0），也有可能为女生（1）。这样正常，因为这个网络还没有经过训练（学习），给出的预测结果并不好。</p>
<p>让我们计算偏导$$\frac{\partial{L}}{\partial{w_1}}$$：<br>$$<br>\boxed{\frac{\partial{L}}{\partial{w_1}} &#x3D; \frac{\partial{L}}{\partial{y_{pred}}} * \frac{\partial{y_{pred}}}{\partial{h_1}} * \frac{\partial{h_1}}{\partial{w_1}}}<br>$$</p>
<p>$$<br>\begin{align}<br>\frac{\partial{L}}{\partial{y_{pred}}} &#x3D;  \frac{\partial{(1 - y_{pred})^2}}{\partial{y_{pred}}} &amp;&#x3D; \boxed{-2(1 - y_{pred})} \<br>&amp;&#x3D; -2 (1 - 0.524) \<br>&amp;&#x3D; -0.952<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>\frac{\partial{y_{pred}}}{\partial{h_1}} &amp;&#x3D; \boxed{w_5 * f’(w_1 x_1 + w_2 x_2 + b_1)} \<br>&amp;&#x3D; 1 * f’(0.0474 + 0.0474 + 0) \<br>&amp;&#x3D; f(0.0948) * (1 - f(0.0948)) \<br>&amp;&#x3D; 0.249<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>\frac{\partial{h_1}}{\partial{w_1}} &amp;&#x3D; \boxed{x1 * f’(w_1 x_1 + w_2 x_2 + b1)} \<br>&amp;&#x3D; -2 * f’(-2 + -1 + 0) \<br>&amp;&#x3D; -2 * f’(-3) * (1 - f(-3)) \<br>&amp;&#x3D; -0.0904<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>\frac{\partial{L}}{\partial{w_1}} &amp;&#x3D; -0.952 * 0.249 * -0.904 \<br>&amp;&#x3D; \boxed{0.0214}<br>\end{align}<br>$$</p>
<blockquote>
<p>其中f’(x)的计算在上文中推导过，就是sigmoid激活函数的导数：$$f’(x) &#x3D; \frac{e^{-x}}{(1 + e^{-x})^2} &#x3D; \boxed{f(x) * (1 - f(x)}$$</p>
</blockquote>
<p>这里也能看出来，如果我们增加权重$$w_1$$，损失函数$$L$$最后会出现一点增长。换句话说，$$w_1$$对最后损失函数$$L$$的贡献度是正的0.0214。</p>
<h3 id="训练：随机梯度下降法"><a href="#训练：随机梯度下降法" class="headerlink" title="训练：随机梯度下降法"></a>训练：随机梯度下降法</h3><p>有了上面的基础，我们可以使用很多<a href="https://www.bilibili.com/video/BV1r64y1s7fU">优化算法</a>可以使用来训练我们的网络。这里我们使用<a href="https://zhuanlan.zhihu.com/p/357963858">随机梯度下降法（Stochastic Gradient Descent, SGD）</a>来优化改变我们的权重和偏置，最后使我们的损失$$L$$最小。（简单易懂，效果不错）</p>
<p>基本上，对$$w_1$$的更新等式：<br>$$<br>w_1 \leftarrow w_1 - \eta \frac{\partial{L}}{\partial{w_1}}<br>$$<br>其中，$$\eta$$是一个设定的常数，称之为<strong>学习率（Learning Rate, LR）</strong>，这个设定的常数可以控制我们训练的速度。上式的意思就是说，在原来的$$w_1$$ 基础上，减去$$\eta \frac{\partial{L}}{\partial{w_1}}$$，最后得到一个新的$$w_1$$。</p>
<ul>
<li>如果$$\frac{\partial{L}}{\partial{w_1}}$$是正的，$$w_1$$将变小，进一步会使损失$$L$$减少（在下次训练的时候）。</li>
<li>如果$$\frac{\partial{L}}{\partial{w_1}}$$是负的，$$w_1$$将增大，进一步也会使损失$$L$$减少（在下次训练的时候）。</li>
</ul>
<p>如果我们对网络中的每个权重和偏置都做相同的运算，那损失将慢慢减少，最后使我们的神经网络得到改善。</p>
<p>总结，我们的整个训练过程将会如下所示：</p>
<ol>
<li>从我们的数据集中选择一个样本，一次只对一个样本进行操作。</li>
<li>计算损失$$L$$对所有的不同权重或偏置求偏导数。（比如，原始的$$L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3, b_4)$$，对其每个变量求偏导$$\frac{\partial{L}}{\partial{w_1}}, \frac{\partial{L}}{\partial{w_2}}, \frac{\partial{L}}{\partial{w_3}}……$$）</li>
<li>使用上面类似的更新公式，来更新每个权重和偏置。</li>
<li>返回到步骤1。</li>
</ol>
<p>不断重复步骤1-4，就是一个不断训练的过程。</p>
<p>OK，下面将理论推导转化为实际的代码。</p>
<h3 id="编写：一个完整的神经网络"><a href="#编写：一个完整的神经网络" class="headerlink" title="编写：一个完整的神经网络"></a>编写：一个完整的神经网络</h3><p>终于，我们可以编写一个完整的神经网络了，同样，继续使用前面的数据集：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Weight (minus 135)</th>
<th>Height (minus 66)</th>
<th>Gender</th>
</tr>
</thead>
<tbody><tr>
<td>Alice</td>
<td>-2</td>
<td>-1</td>
<td>1</td>
</tr>
<tr>
<td>Bob</td>
<td>25</td>
<td>6</td>
<td>0</td>
</tr>
<tr>
<td>Charlie</td>
<td>17</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>Diana</td>
<td>-15</td>
<td>-6</td>
<td>1</td>
</tr>
</tbody></table>
<p>继续使用前面的神经网络结构：</p>
<p><img data-src="https://cheery-markdown-img.oss-cn-chengdu.aliyuncs.com/blog/20230105221004.png" alt="带权重和偏置的神经网络"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="comment"># Sigmoid activation function: f(x) = 1 / (1 + e^(-x))</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deriv_sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="comment"># Derivative of sigmoid: f&#x27;(x) = f(x) * (1 - f(x))</span></span><br><span class="line">  fx = sigmoid(x)</span><br><span class="line">  <span class="keyword">return</span> fx * (<span class="number">1</span> - fx)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">  <span class="comment"># y_true and y_pred are numpy arrays of the same length.</span></span><br><span class="line">  <span class="keyword">return</span> ((y_true - y_pred) ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OurNeuralNetwork</span>:</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  A neural network with:</span></span><br><span class="line"><span class="string">    - 2 inputs</span></span><br><span class="line"><span class="string">    - a hidden layer with 2 neurons (h1, h2)</span></span><br><span class="line"><span class="string">    - an output layer with 1 neuron (o1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  *** DISCLAIMER ***:</span></span><br><span class="line"><span class="string">  The code below is intended to be simple and educational, NOT optimal.</span></span><br><span class="line"><span class="string">  Real neural net code looks nothing like this. DO NOT use this code.</span></span><br><span class="line"><span class="string">  Instead, read/run it to understand how this specific network works.</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># Weights</span></span><br><span class="line">    self.w1 = np.random.normal()</span><br><span class="line">    self.w2 = np.random.normal()</span><br><span class="line">    self.w3 = np.random.normal()</span><br><span class="line">    self.w4 = np.random.normal()</span><br><span class="line">    self.w5 = np.random.normal()</span><br><span class="line">    self.w6 = np.random.normal()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Biases</span></span><br><span class="line">    self.b1 = np.random.normal()</span><br><span class="line">    self.b2 = np.random.normal()</span><br><span class="line">    self.b3 = np.random.normal()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># x is a numpy array with 2 elements.</span></span><br><span class="line">    h1 = sigmoid(self.w1 * x[<span class="number">0</span>] + self.w2 * x[<span class="number">1</span>] + self.b1)</span><br><span class="line">    h2 = sigmoid(self.w3 * x[<span class="number">0</span>] + self.w4 * x[<span class="number">1</span>] + self.b2)</span><br><span class="line">    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)</span><br><span class="line">    <span class="keyword">return</span> o1</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, data, all_y_trues</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    - data is a (n x 2) numpy array, n = # of samples in the dataset.</span></span><br><span class="line"><span class="string">    - all_y_trues is a numpy array with n elements.</span></span><br><span class="line"><span class="string">      Elements in all_y_trues correspond to those in data.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    learn_rate = <span class="number">0.1</span></span><br><span class="line">    epochs = <span class="number">1000</span> <span class="comment"># number of times to loop through the entire dataset</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">      <span class="keyword">for</span> x, y_true <span class="keyword">in</span> <span class="built_in">zip</span>(data, all_y_trues):</span><br><span class="line">        <span class="comment"># --- Do a feedforward (we&#x27;ll need these values later)</span></span><br><span class="line">        sum_h1 = self.w1 * x[<span class="number">0</span>] + self.w2 * x[<span class="number">1</span>] + self.b1</span><br><span class="line">        h1 = sigmoid(sum_h1)</span><br><span class="line"></span><br><span class="line">        sum_h2 = self.w3 * x[<span class="number">0</span>] + self.w4 * x[<span class="number">1</span>] + self.b2</span><br><span class="line">        h2 = sigmoid(sum_h2)</span><br><span class="line"></span><br><span class="line">        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3</span><br><span class="line">        o1 = sigmoid(sum_o1)</span><br><span class="line">        y_pred = o1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --- Calculate partial derivatives.</span></span><br><span class="line">        <span class="comment"># --- Naming: d_L_d_w1 represents &quot;partial L / partial w1&quot;</span></span><br><span class="line">        d_L_d_ypred = -<span class="number">2</span> * (y_true - y_pred)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Neuron o1</span></span><br><span class="line">        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)</span><br><span class="line">        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)</span><br><span class="line">        d_ypred_d_b3 = deriv_sigmoid(sum_o1)</span><br><span class="line"></span><br><span class="line">        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)</span><br><span class="line">        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Neuron h1</span></span><br><span class="line">        d_h1_d_w1 = x[<span class="number">0</span>] * deriv_sigmoid(sum_h1)</span><br><span class="line">        d_h1_d_w2 = x[<span class="number">1</span>] * deriv_sigmoid(sum_h1)</span><br><span class="line">        d_h1_d_b1 = deriv_sigmoid(sum_h1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Neuron h2</span></span><br><span class="line">        d_h2_d_w3 = x[<span class="number">0</span>] * deriv_sigmoid(sum_h2)</span><br><span class="line">        d_h2_d_w4 = x[<span class="number">1</span>] * deriv_sigmoid(sum_h2)</span><br><span class="line">        d_h2_d_b2 = deriv_sigmoid(sum_h2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --- Update weights and biases</span></span><br><span class="line">        <span class="comment"># Neuron h1</span></span><br><span class="line">        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1</span><br><span class="line">        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2</span><br><span class="line">        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Neuron h2</span></span><br><span class="line">        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3</span><br><span class="line">        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4</span><br><span class="line">        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Neuron o1</span></span><br><span class="line">        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5</span><br><span class="line">        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6</span><br><span class="line">        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3</span><br><span class="line"></span><br><span class="line">      <span class="comment"># --- Calculate total loss at the end of each epoch</span></span><br><span class="line">      <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        y_preds = np.apply_along_axis(self.feedforward, <span class="number">1</span>, data)</span><br><span class="line">        loss = mse_loss(all_y_trues, y_preds)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch %d loss: %.3f&quot;</span> % (epoch, loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define dataset</span></span><br><span class="line">data = np.array([</span><br><span class="line">  [-<span class="number">2</span>, -<span class="number">1</span>],  <span class="comment"># Alice</span></span><br><span class="line">  [<span class="number">25</span>, <span class="number">6</span>],   <span class="comment"># Bob</span></span><br><span class="line">  [<span class="number">17</span>, <span class="number">4</span>],   <span class="comment"># Charlie</span></span><br><span class="line">  [-<span class="number">15</span>, -<span class="number">6</span>], <span class="comment"># Diana</span></span><br><span class="line">])</span><br><span class="line">all_y_trues = np.array([</span><br><span class="line">  <span class="number">1</span>, <span class="comment"># Alice</span></span><br><span class="line">  <span class="number">0</span>, <span class="comment"># Bob</span></span><br><span class="line">  <span class="number">0</span>, <span class="comment"># Charlie</span></span><br><span class="line">  <span class="number">1</span>, <span class="comment"># Diana</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train our neural network!</span></span><br><span class="line">network = OurNeuralNetwork()</span><br><span class="line">network.train(data, all_y_trues)</span><br></pre></td></tr></table></figure>

<p>最后结果可以看到：随着网络的学习，我们的损失逐步减少：</p>
<img data-src="./CNN初解.assets/image-20230106100542914.png" alt="image-20230106100542914" style="zoom: 50%;" />

<p>使用训练好的网络，再来进行性别预测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Make some predictions</span></span><br><span class="line">emily = np.array([-<span class="number">7</span>, -<span class="number">3</span>]) <span class="comment"># 128 pounds, 63 inches</span></span><br><span class="line">frank = np.array([<span class="number">20</span>, <span class="number">2</span>])  <span class="comment"># 155 pounds, 68 inches</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Emily: %.3f&quot;</span> % network.feedforward(emily)) <span class="comment"># 0.951 - F</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Frank: %.3f&quot;</span> % network.feedforward(frank)) <span class="comment"># 0.039 - M</span></span><br></pre></td></tr></table></figure>

<p>最后，可以看到我们的神经网络已经能比较正确分辨体重和身高特征较为明显的男生女生。</p>
<ul>
<li><p><strong>Reference</strong></p>
<p><a href="https://victorzhou.com/blog/intro-to-neural-networks/">intro-to-neural-networks</a></p>
</li>
</ul>
<p>PCA，SVD: 矩阵运算，旋转拉伸。</p>
<p>CNN: 卷积、信号处理、傅里叶变换。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>建站历史回顾</title>
    <url>/2022/09/04/%E5%BB%BA%E7%AB%99%E5%8E%86%E5%8F%B2%E5%9B%9E%E9%A1%BE/</url>
    <content><![CDATA[<h2 id="建站历史"><a href="#建站历史" class="headerlink" title="建站历史"></a>建站历史</h2><p><img data-src="/images/%E5%BB%BA%E7%AB%99%E5%8E%86%E5%8F%B2%E5%9B%9E%E9%A1%BE/history.png"></p>
<p>2021年2月:</p>
<p>第一次使用Hexo进行建站，陆续花了3个晚上摸索前端基础，并找出了蒙尘了3年之久的树莓派3B套件（看到大佬稚晖君自己用小电视建站才想起来）。通过花生壳内网穿透，购买自己的域名，浏览器输入cheerychen.com进行连接，加载出Hello World那刻，心花怒放，傻笑了好久。</p>
<span id="more"></span>

<hr>
<p>2021年2月–2022年9月:</p>
<p>从MediaTek辞职，成功上岸稚晖君的母校读博。</p>
<hr>
<p>2022年9月</p>
<p>博士开学第一周，倒了一年半的站，重新拉起来，记录在重新读书期间的学习过程，技术笔记等。</p>
<blockquote>
<p>建站容易，维护不易，一定坚持维护！</p>
</blockquote>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>网站</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数笔记1</title>
    <url>/2022/09/20/%E3%80%8A%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AF%BC%E8%AE%BA%E3%80%8B%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h1 id="test-title"><a href="#test-title" class="headerlink" title="test title"></a>test title</h1>]]></content>
      <categories>
        <category>数学基础</category>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
</search>
